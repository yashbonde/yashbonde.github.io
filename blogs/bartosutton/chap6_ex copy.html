<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">

    <title>Barto Sutton | Chapter 7 Exercises</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:100,400,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:300,400,400i,600,600i,700,700i,800" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,400i,700,700i" rel="stylesheet">

    <!-- Link the main.css stylesheet -->
    <link href="../styles/reset.css" rel="stylesheet">
    <link href="../styles/-debug.css" rel="stylesheet">
    <link href="../styles/article.css" rel="stylesheet">
    <link href="../styles/article-text.css" rel="stylesheet">
    <link href="../styles/article-figure.css" rel="stylesheet">
    <link href="../styles/nav.css" rel="stylesheet">
    <link href="../styles/code-block.css" rel="stylesheet">
    <link href="./exer.css" rel="stylesheet">

    <link href="../styles/footer-blog.css" rel="stylesheet">

    <!-- Latex -->
    <!-- for help with symbols goto: https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

</head>
<body>
    <nav>
        <a class = "nav-home" href="../../index.html">HOME</a>
        <a class = "nav-project" href="../../projects.html">PROJECTS</a>
        <a class = "nav-blog" href="../../musings.html">BLOG</a>
        <a class = "nav-resume" href="../../files/yashbonde_resume.pdf">RESUME</a>
    </nav>

    <article id = "blog-0">

        <h1>Chapter 6 Exercises</h1>
        <h2>Some solutions might be off</h2>

        <time datetime="27-04-2019">APRIL 27, 2020</time>

        <p>
            <b>NOTE: This part requires some basic understading of calculus.</b><br>
            These are just my solutions of the book <a href="http://incompleteideas.net/book/the-book-2nd.html" class="article-a">Reinforcement Learning: An Introduction</a>, all the credit for book goes to the authors and other contributors. Complete notes can be found <a href="./chap6.html" class="article-a">here</a>. If there are any problems with teh solutions or you have some ideas ping me at <code>bonde.yash97@gmail.com</code>.
        </p>

        <h3 id="6_1">6.1 Changing Value function</h3>
        <p>
            \[G_t - V_t(S_t) = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_t(S_{t+1}) -\gamma V_t(S_{t+1}) \]
            \[= \delta_t + \gamma (G_{t+1} - V_t(S_{t+1}))\]
            \[= \delta_t + \gamma (G_{t+1} - V_{t+1}(S_{t+1})) + \gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1}))\]
            Now notice that \(V_{t+1}(S_{t+1}) - V_t(S_{t+1})\) is the update with value \(u_t = \alpha(R_t + \gamma V(S_{t+1}) - V(S_t))\) so we can write this as
            \[= \delta_t + \gamma (G_{t+1} - V_{t+1}(S_{t+1})) + \gamma u_{t+1}\]
            expanding and compressing gives us
            \[= \sum_{k=0}^{T-1}[\gamma^{k-t}\delta_k + \gamma^{k-t+1}u_{k-t+1}]\]
            So the amount we have added is \(\sum_{k=0}^{T-1}\gamma^{k-t+1}u_{k-t+1}\)
        </p>

        <h3 id="6_2">6.2 TD vs. MC</h3>
        <p>
            I think alot of problem that require an estimation for long sequences where we need the estimation as the task continues] would benefit from the TD learning system. Eg. we all love to eat and often have tendency to overeat, if we decide to go to a buffet and our priorities is to eat diversity of food while enjoying the good ones more. This is a perfect setup where TD learning would be beneficial.
        </p>

        <h3 id="6_3">6.3 Analysis of random walks</h3>
        <p>
            We know that for the first step only \(V(A)\) was changed which means the random walk landed on \(A\) and from there it went to the terminal state on left, other wise the \(V(E)\) would have changed. Again, because walk ended on terminal state on left of \(A\) only \(V(A)\) changed. We can calculate the values very easily:
            \[V(A)_{t+1} = V(A)_t + \alpha (R + \gamma V(Term) - V(A)_t)\]
            Now we have \(V(Term) = 0\), \(R = 0\) and \(\alpha = 0.1\), so we get \(V(A) = 0.45\). Similarly if it went the other way \(V(E) = 5.5\).
        </p>

        <h3 id="6_4">6.4 Analysis of random walks</h3>
        <p>
            It is clear that if there was a wider set of \(\alpha\)s then TD would be a better algorithm than MC. As seen that we get good performance for TD at \(\alpha = 0.05\) and at \(\alpha = 0.04\) the MC starts to detoriate. So even though the TD learning would occur when \(\alpha\) is small it would become sub-optimal as it would take more walks per episode to learn.
        </p>

        <h3 id="6_5">6.5 Analysis of random walks</h3>
        <p>
            We can see that increase in \(\alpha\) increases the MSE over multiple steps that might happend because the noise is too high. You think like it starts to oscillate around the true value function.
        </p>

        <h3 id="6_8">6.8 Sarsa Convergece</h3>
        <p>
            \[G_t - Q(S_t, A_t) = R_{t+1} + \gamma G_{t+1} - Q(S_t, A_t) + \gamma Q(S_{t+1}, A_{t+1}) -\gamma Q(S_{t+1}, A_{t+1}) \]
            \[= \delta_t + \gamma (G_{t+1} - Q(S_{t+1}, A_{t+1}))\]
            we can now continue to substitute the values and see the expansion as and \(Q(S_T, A_T) = 0; G_T = 0\)
            \[= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T -Q(S_T, A_T))\]
            \[= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0 - 0)\]
            \[\delta^{TD}_t = \sum_{k=t}^{T-t}\gamma^{k-t}\delta_k\]
        </p>
        
        <h3 id="6_11">6.11 Q-learning</h3>
        <p>
            There is a subtle difference in the Sarsa and Q-learning, ie. the action taken by Sarsa is the next action taken (\(S \leftarrow S'; A \leftarrow A'\)) where as in Q-learning the only the state changes (\(S \leftarrow S'\)) and new action is taken at each step. Thus learning happens independent of whether the action was taken or not!
        </p>

        <h3 id="6_12">6.12 Q-learning \(\approx\) Sarsa \(?\) </h3>
        <p>
            The question is if we make the action selection greedy then are the two algorithms same. The answer is no, consider this case \(Q(S,a) = [0.1, 0.09 ... 0.01]\), now considering greedy action selection we get \(A = 1\). On observing the followed rewards and new state \(R, S'\) we update the value of this state so say that we got a large negative reward then the new values may become \(Q(S,a) = [0.009, 0.09 ... 0.01]\).
        </p>
        <p>
            <b>Action:</b> In case of Sarsa we will see that the action is taken as \(A \leftarrow A'\) and so the action cannot change, however the value is again updated for Q-learning and this time it will not choose \(A = 1\). Thus no the action for the two will not be same.
        </p>

        <p>
            <b>Weight Update:</b> They will be same as we basically have the same variables, thus if both algorithms take action \(A\) and get the same \(R\) and \(S'\) we do get the same weight updates.
        </p>

    </article>

    <footer>
        <p class="footer-p">
            CONNECT WITH ME
        </p>
        <div class = "share">
            <a href="https://www.linkedin.com/in/yash-bonde/"><img src="../images/linkedin.svg"></a>
            <a href="https://www.instagram.com/yassh.bonde/"><img src="../images/instagram.svg"></a>
        </div>
    </footer>

</body>
</html>