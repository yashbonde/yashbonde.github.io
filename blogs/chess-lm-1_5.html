<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">

    <title>Chessssssss</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:100,400,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:300,400,400i,600,600i,700,700i,800" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,400i,700,700i" rel="stylesheet">

    <!-- Link the main.css stylesheet -->
    <link href="styles/reset.css" rel="stylesheet">
    <link href="styles/-debug.css" rel="stylesheet">
    <link href="styles/article.css" rel="stylesheet">
    <link href="styles/article-text.css" rel="stylesheet">
    <link href="styles/article-figure.css" rel="stylesheet">
    <link href="styles/nav.css" rel="stylesheet">
    <link href="styles/code-block.css" rel="stylesheet">

    <link href="styles/footer-blog.css" rel="stylesheet">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

    <style type="text/css">
      .gist-data {max-height: 400px}
    </style>

</head>
<body>
    <nav>
        <a class = "nav-home" href="../index.html">HOME</a>
        <a class = "nav-project" href="../projects.html">PROJECTS</a>
        <a class = "nav-blog" href="../musings.html">BLOG</a>
        <a class="nav-resume" href="../files/yashbonde_resume.pdf">RESUME</a>
    </nav>

    <article id = "blog-0">

        <h1>Chess Engine - (Part 1.5)</h1>
        <h2>On scaling laws for Language Models</h2>

        <time datetime="23-11-2020">NOV. 23, 2020</time>

        <p>
            This is first in a series of blogs covering this project. <strong>TL;DR</strong> see this
            <a href="https://github.com/yashbonde/chess_lm" class="article-a">Github repo</a>.
            Also for all you who do not understand the Indian number system just read this
            <a href="https://en.wikipedia.org/wiki/Indian_numbering_system" class="article-a">wiki</a>.
            This is a follow up to my <a href="./chess-lm.htmlx class="article-a">previous work</a> where
            I train a language model on \(25Cr\) or \(225.1Mn\) moves and pose some questions.
        </p>

        <p>
            If you have any freelancing work, contact me on my <a
            href="https://www.linkedin.com/in/yash-bonde/" class="article-a">LinkedIn</a>.
        </p>

        <p>
            Initially this was going to be do and done kind of project but now it seems like this is going to take
            a while and if I am lucky I might just get a paper out of it. There are so many shitty papers out there
            that you can't believe something so meaningless can get a paper out. But anyways, it's them and we gotta
            do our à¤¯à¥‹à¤—. This blog is mostly a paper discussion and will be a quick one.
        </p>
        
        <h2 id="power-laws">ðŸ“¶ What are power laws</h2>

        <p>
            It used to be when compute was low that simply adding more layers to a network used to work. But this
            paradigm changed with very large model (GPT-2, BERT, T5, etc.) and with the investment in these softwares
            it became important to know how these model actually scale, what matters and what not. So OpenAI did a
            research and answered those questions based on <a href="https://en.wikipedia.org/wiki/Power_law"
            class="article-a">Power Law</a>. So we start our discussion with what is a powerlaw.
        </p>

        <p>
            Powerlaws are mathematical relationship where changing one value causes a proportional change in the outcome or
            power law is a function \(f(x)\) where the value \(y\) is proportional to some value of \(x\).
            Eg. doubling the length makes area \(4\) times and volume \(8\) times. So we can give it a form \(A(x) \propto x^k\)
            where \(k = 2\) and when considering volume \(k = 3\). Such functions have three properties:
            <li><b>Scale Invariance:</b> given a relation \(f(x) = ax^{-\alpha}\) scaling \(x\) by a factor \(c\) causes
                proportional scaling of the function itself ie. \(f(cx) = c^{-\alpha} f(x)\). Thus the log of these values
                gives a linear line.
            <li>Lack of well defined average value, which makes it hard to apply traditional statistics like mean, etc. An
                example if the <a class="article-a" href="https://en.wikipedia.org/wiki/Pareto_distribution">
                Pareto Distribution</a>.
            <li>Universality, which in gist means that as a different systems behave the same way. And the exponent in such
                a distribution (\(k\)) remains the same for those different system. In the paper it means that for a lot of
                different models the scaling parameters eg. \(\alpha_N, \alpha_D\) remain the same for large number of values.
        </p>

        <figure class="size-2">
            <img src="./images/chess-lm-4.png">
        </figure>
        <figcaption>
            <p style="font-size:0.9rem; text-align:centre">(Left) you have normal power law distribution and (Right) shows
                that scaling x-axis to log gives a linear graph. Which helps when working exponentially increasing/decreasing
                values. \(y = f(x) = 5.6 x^{-0.095} \forall x \in [0,1000) \)</p>
        </figcaption>

        <h2 id="on-intelligence">ðŸ“¶ How to make network bigger (PROPERLY)</h2>

        <p>
            The paper we are dealing with today is called "Scaling Laws for Neural Language Models"
            (<a href="http://arxiv.org/abs/2001.08361" class="article-a">ArXiv</a>) by OpenAI. With the knowledge of the power
            laws it becomes easier to predict things at scale and this is helpful as neural networks grow exponentially in size.
            But we start with defining the nomenclature:
            <li> \(L\): Cross entropy loss in <a href="https://en.wikipedia.org/wiki/Nat_(unit)" class ="article-a">nats</a>,
                typically it will be averaged over all the tokens in the context.
            <li> \(N\): The number of models parameters, excluding all the vocabulary and positional parameters. This can
                roughly be calculated as \(N \approx 2d_{model}n_{layer}(2 d_{attn} + d_{ff}) = 12n_{layer}d_{model}^2\)
            <li> \(D\): The dataset size in tokens
            <li> \(C\): Compute required. One forward pass takes \(C_{forward} \approx 2N + 2d_{model}n_{layer}n_{ctx}\)
                where the factor \(2\) comes from multiply-accumuate operation used in matrix multiplication. Accounting for
                the backwards pass (twice the forward pass) the total compute comes at \(C \approx 6N\) per training token.
            <li> \(B_{crit}\): The critical batch size. Training at the critical batch size provides a roughly optimize
                between time and compute efficiency.
            <li> \(C_{min}\): An estimate of the minimum amount of non-embedding compute to reach a given value of the loss
                (batch size \(\ll\) critical batch size)
            <li> \(S_{min}\): An estimate of the minimal number of training steps needed to reach a given value of the loss
                (batch size \(\ll\) critical batch size)
            <li> \(\alpha_X\): power law exponents for scaling of the loss \(L(X) \propto 1/X^{\alpha_X}\) where \(X\) can be
                any of \(N,D,C,S,B,C^{min}\)
        </p>

        <figure class="size-2">
            <img src="./images/chess-lm-4.png">
        </figure>
        <figcaption>
            <p style="font-size:0.9rem; text-align:centre">Parameter counts and compute (forward pass) estimates for a
                Transformer model. Sub-leading terms such as nonlinearities, biases, and layer normalization
                are omitted.</p>
        </figcaption>

        <p>
            What are the lesson learnt, those with images are given below the bullet points:
            <li> Transformer performance depends very weakly on the shape parameters \(n_{layer}, n_{heads}\) and \(d_{ff}\)
                when we hold the non-embedding parameters count fixed to \(N\).
            <li> The variation in the loss with different random seeds is roughly \(0.02\), does this mean setting seeds during
                training is pretty much useless?
            <li> To avoid overfitting when training within the above threshold of convergence we require
                \(D \geq (5 \times 10^3) N^{0.74}\). However, this according to my calculations actually comes
                \(D \geq (1.73 \times 10^3) N^{0.74}\)
                <!-- <a href="diff-1" class="article-a">here</a> -->.
            <li> Convergence is inefficient: When working with a fixed compute \(C\) but without any other restrictions on \(N\) and
                \(D\), we attain optimal performance by training a very large network and stopping significantly before convergence.
                As given by relation \(D \sim C^{0.27}\)
            <li> The performance penalty depends predictably on the ratio \(N^{0.74}/D\), meaning that every time we increase the
                model size \(8\times\), we only need to increase the data by roughly \(5\times\) to avoid a penalty.
            <li> This is somewhat obvious but larger models are more sample efficient, reaching same performance with fewer parameters.
        </p>

        <figure class="size-2">
            <img src="./images/chess-lm-6.png">
        </figure>
        <figcaption>
            <p style="font-size:0.9rem; text-align:centre"> (left) It means that for a given compute size there is an optimal
                model size (where the loss is lowest, on the line) and for the same amout of compute both the larger and the
                smaller models can give the same results (test loss). Thus we should find the optimal model for this compute size.
                (right) at same steps, larger models give lower loss and on increasing the steps there is decrease in loss for
                the same model.
            </p>
        </figcaption>

        <figure class="size-2">
            <img src="./images/chess-lm-7.png">
        </figure>
        <figcaption>
            <p style="font-size:0.9rem; text-align:centre"> This has to be the best representation of results. What this shows
                is that for each variable shown, if the other two are optimal then the test loss is very predictable and follows
                smooth curves. Thus in order to train an efficient model we need to grow all three in tandem. This is the
                engineering challenge!
            </p>
        </figcaption>

        <figure class="size-2">
            <img src="./images/chess-lm-8.png">
        </figure>
        <figcaption>
            <p style="font-size:0.9rem; text-align:centre"> This is another important graph. On the right you see that datasize
                can also become a bottleneck (ðŸ˜“ this sounds familiar), ie. when you have a small dataset even larger models can
                suffer same loss. When this happens no amount of compute or model size increase can help you and you start to
                move away from efficient frontier. On the right is the 
                For reference:
                \(L(N,D) = \Big( (\frac{N_C}{N})^{\frac{\alpha_N}{\alpha_D}} + \frac{D_C}{D} \Big)^{\alpha_D}\) and
                on right, X-axis is \(N^{0.737}/D\).
            </p>
        </figcaption>


        <h3>So what is the problem?</h3>

        <blockquote style="border-left: 8px solid #ffbf00;">
            Kolmogorov defined the complexity of a string \(\text{x}\) as the length of its shortest description \(\text{p}\)
            on a universal Turing machine \(\text{U(K(x))} = \min\{\text{l(p):U(p)=x}\}\), .
        </blockquote>

        <!-- <p id="diff-1">
            <b>Data threshold for convergence:</b> (I am pretty sure I have done some mistake here) According the formula
            \[\delta L = \frac{L(N,D)}{L(N,\infty)} - 1\]
            We can insert the expansion of \(L(N,D)\) and \(L(N,\infty)\) and get the value
            \[\delta L = \frac{\Big( (\frac{N_C}{N})^{\frac{\alpha_N}{\alpha_D}} + \frac{D_C}{D} \Big)^{\alpha_D}}
            {\Big( \frac{N_C}{N} \Big)^{\frac{\alpha_N}{\alpha_D}}} - 1\]
            \[\delta L \approx \Big( 1 + \Big( \frac{N}{N_C} \Big)^{\frac{\alpha_N}{\alpha_D}} \frac{D_C}{D} \Big)^{\alpha_D} -1 \]
            Feeding values for \(N_C, \alpha_N, \alpha_D, D_C\) I got value as
            \[0.9798 \approx \]
        </p>
         -->
    </article>

    <footer>
        <p class="footer-p">
            CONNECT WITH ME
        </p>
        <div class = "share">
            <a href="https://www.linkedin.com/in/yash-bonde/"><img src="images/linkedin.svg"></a>
            <a href="https://www.instagram.com/yassh.bonde/"><img src="images/instagram.svg"></a>
        </div>
    </footer>

</body>
</html>